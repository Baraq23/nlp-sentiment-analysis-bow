{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266c2735-0190-4ef2-9ead-fc8694f1dca0",
   "metadata": {},
   "source": [
    "# Exercise 7: Bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef28ad1e-adb9-4e29-a848-2932de011550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc980e31-d861-4a23-b9fc-e523c0b35fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function from Exercise 6\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06f40c4b-bc09-476d-95a3-fad43063bdd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6588 tweets\n",
      "\n",
      "First 3 tweets:\n",
      "1. [1] Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sa...\n",
      "2. [-1] Theo Walcott is still shit, watch Rafa and Johnny deal with ...\n",
      "3. [-1] its not that I'm a GSP fan, i just hate Nick Diaz. can't wai...\n",
      "\n",
      "Example preprocessing:\n",
      "Original: Gas by my house hit $3.39!!!! I'm going to Chapel Hill on Sat. :)\n",
      "Preprocessed: ga hous hit 339 im go chapel hill sat\n"
     ]
    }
   ],
   "source": [
    "# Load tweets from file\n",
    "def load_tweets(filename):\n",
    "    \"\"\"\n",
    "    Load tweets from tweets_train.txt\n",
    "    Format: sentiment, tweet_text\n",
    "    \"\"\"\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    \n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                # Split only on first comma to preserve commas in tweet text\n",
    "                parts = line.split(', ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    sentiment, tweet = parts\n",
    "                    tweets.append(tweet)\n",
    "                    \n",
    "                    # Convert sentiment to numeric label\n",
    "                    if sentiment == 'positive':\n",
    "                        labels.append(1)\n",
    "                    elif sentiment == 'negative':\n",
    "                        labels.append(-1)\n",
    "                    else:  # neutral\n",
    "                        labels.append(0)\n",
    "    \n",
    "    return tweets, labels\n",
    "# data_url = \"https://learn.zone01kisumu.ke/api/content/root/public/subjects/ai/nlp/resources/tweets_train.txt\"\n",
    "# Load the data\n",
    "tweets, labels = load_tweets('tweets_train.txt')\n",
    "\n",
    "print(f\"Loaded {len(tweets)} tweets\")\n",
    "print(f\"\\nFirst 3 tweets:\")\n",
    "for i in range(3):\n",
    "    print(f\"{i+1}. [{labels[i]}] {tweets[i][:60]}...\")\n",
    "\n",
    "# Preprocess all tweets\n",
    "preprocessed_tweets = [preprocess_text(tweet) for tweet in tweets]\n",
    "\n",
    "print(f\"\\nExample preprocessing:\")\n",
    "print(f\"Original: {tweets[0]}\")\n",
    "print(f\"Preprocessed: {preprocessed_tweets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c6f6dde-7b59-4ec4-a776-f4082fa94d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape of word count matrix: (6588, 500)\n",
      "(rows: 6588 tweets, columns: 500 features)\n"
     ]
    }
   ],
   "source": [
    "# Create CountVectorizer with max_features=500\n",
    "vectorizer = CountVectorizer(max_features=500)\n",
    "X = vectorizer.fit_transform(preprocessed_tweets)\n",
    "\n",
    "print(f\"\\nShape of word count matrix: {X.shape}\")\n",
    "print(f\"(rows: {X.shape[0]} tweets, columns: {X.shape[1]} features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54cbc2c2-866d-4f5c-b909-0a93f0c981d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word count DataFrame (first 5 rows, first 10 columns):\n",
      "   10  10th  12  15  15th  16  19th  1st  2012  22\n",
      "0   0     0   0   0     0   0     0    0     0   0\n",
      "1   0     0   0   0     0   0     0    0     0   0\n",
      "2   0     0   0   0     0   0     0    0     0   0\n",
      "3   0     0   0   0     0   0     0    0     0   0\n",
      "4   0     0   0   0     0   0     0    0     0   0\n"
     ]
    }
   ],
   "source": [
    " # Create DataFrame from sparse matrix\n",
    "count_vectorized_df = pd.DataFrame.sparse.from_spmatrix(\n",
    "    X, \n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "print(\"\\nWord count DataFrame (first 5 rows, first 10 columns):\")\n",
    "print(count_vectorized_df.iloc[:5, :10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dae80cd4-f40f-42a9-8892-e210439a4674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Token counts of the 4th tweet ---\n",
      "Original tweet: Iranian general says Israel's Iron Dome can't deal with their missiles (keep talking like that and we may end up finding out)\n",
      "\n",
      "Word counts:\n",
      "cant    1\n",
      "deal    1\n",
      "end     1\n",
      "find    1\n",
      "keep    1\n",
      "like    1\n",
      "may     1\n",
      "say     1\n",
      "talk    1\n",
      "Name: 3, dtype: Sparse[int64, 0]\n"
     ]
    }
   ],
   "source": [
    "# Show token counts of the 4th tweet (index 3)\n",
    "print(\"\\n--- Token counts of the 4th tweet ---\")\n",
    "fourth_tweet_counts = count_vectorized_df.iloc[3][count_vectorized_df.iloc[3] > 0]\n",
    "print(f\"Original tweet: {tweets[3]}\")\n",
    "print(f\"\\nWord counts:\")\n",
    "print(fourth_tweet_counts.sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8530872-2c44-4d8f-a5ae-3ca4a72ae917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 15 most used tokenized words ---\n",
      "tomorrow    1126\n",
      "go           733\n",
      "day          667\n",
      "night        641\n",
      "may          533\n",
      "tonight      501\n",
      "see          439\n",
      "time         429\n",
      "im           422\n",
      "get          398\n",
      "today        389\n",
      "game         382\n",
      "saturday     379\n",
      "friday       375\n",
      "sunday       368\n",
      "dtype: Sparse[int64, 0]\n"
     ]
    }
   ],
   "source": [
    "# Show 15 most used words across all tweets\n",
    "word_counts = count_vectorized_df.sum(axis=0).sort_values(ascending=False)\n",
    "print(\"\\n--- 15 most used tokenized words ---\")\n",
    "print(word_counts.head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f32325d5-821f-40f7-91d2-d431ba6995bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final DataFrame with labels ---\n",
      "   10  10th  12  15  15th  16  19th  1st  2012  22  ...  would  yall  ye  \\\n",
      "0   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "1   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "2   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "3   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "4   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "5   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "6   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "7   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "8   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "9   0     0   0   0     0   0     0    0     0   0  ...      0     0   0   \n",
      "\n",
      "   year  yesterday  yet  york  young  your  label  \n",
      "0     0          0    0     0      0     0      1  \n",
      "1     0          0    0     0      0     0     -1  \n",
      "2     0          0    0     0      0     0     -1  \n",
      "3     0          0    0     0      0     0     -1  \n",
      "4     0          0    0     0      0     0      1  \n",
      "5     0          0    0     0      0     0     -1  \n",
      "6     0          0    0     0      0     0      0  \n",
      "7     0          0    0     0      0     0     -1  \n",
      "8     0          0    0     0      0     0      0  \n",
      "9     0          0    0     0      0     0      0  \n",
      "\n",
      "[10 rows x 501 columns]\n"
     ]
    }
   ],
   "source": [
    "# Add label column\n",
    "count_vectorized_df['label'] = labels\n",
    "\n",
    "print(\"\\n--- Final DataFrame with labels ---\")\n",
    "print(count_vectorized_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "855d5515-a828-4956-a738-32eba228620a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Label distribution ---\n",
      "label\n",
      " 0    3236\n",
      " 1    2413\n",
      "-1     939\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Positive: 2413\n",
      "Neutral: 3236\n",
      "Negative: 939\n",
      "\n",
      "--- Sample rows from final dataset ---\n",
      "   label  10  10th  12  15  15th\n",
      "0      1   0     0   0   0     0\n",
      "1     -1   0     0   0   0     0\n",
      "2     -1   0     0   0   0     0\n",
      "3     -1   0     0   0   0     0\n",
      "4      1   0     0   0   0     0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Label distribution ---\")\n",
    "label_dist = count_vectorized_df['label'].value_counts()\n",
    "print(label_dist)\n",
    "print(f\"\\nPositive: {label_dist.get(1, 0)}\")\n",
    "print(f\"Neutral: {label_dist.get(0, 0)}\")\n",
    "print(f\"Negative: {label_dist.get(-1, 0)}\")\n",
    "\n",
    "# Show sample of final structure\n",
    "print(\"\\n--- Sample rows from final dataset ---\")\n",
    "print(count_vectorized_df[['label']].join(\n",
    "    count_vectorized_df.iloc[:, :5]\n",
    ").head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc812b-f344-4061-b398-647a51502d51",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
