{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e15ac46-7f4e-48fb-8f7d-e55a9c9fe59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7338bbdd-2663-4d95-aa1e-d94a3829e1b2",
   "metadata": {},
   "source": [
    "# Exercise 1: Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85765274-9351-49e2-9dc2-ff46c68b7a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    This is my first NLP exercise\n",
      "1                         wtf!!!!!\n",
      "Name: text, dtype: object\n",
      "Lowercase text\n",
      " ['this is my first nlp exercise', 'wtf!!!!!']\n",
      "Uppercase text\n",
      " ['THIS IS MY FIRST NLP EXERCISE', 'WTF!!!!!']\n"
     ]
    }
   ],
   "source": [
    "# sample data\n",
    "list_ = [\"This is my first NLP exercise\", \"wtf!!!!!\"]\n",
    "series_data = pd.Series(list_, name='text')\n",
    "\n",
    "print(series_data)\n",
    "print(\"Lowercase text\\n\", series_data.str.lower().tolist())\n",
    "print(\"Uppercase text\\n\", series_data.str.upper().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3202e6-8df3-4837-8274-ad486d336c66",
   "metadata": {},
   "source": [
    "# Exercise 2: punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3d742f3-4d4d-4d26-9990-ff2a49d92199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned sentence:\n",
      "Remove this from  the sentence  \n",
      "\n",
      "Alternative method:\n",
      "Remove this from  the sentence  \n"
     ]
    }
   ],
   "source": [
    "sentence = \"Remove, this from .? the sentence !!!! !\"#&'()*+,-./:;<=>_\"\n",
    "\n",
    "# string.punctuation contains all punctuation marks\n",
    "cleaned = sentence.translate(str.maketrans('', '', string.punctuation))\n",
    "print(\"Cleaned sentence:\")\n",
    "print(cleaned)\n",
    "\n",
    "# Alternatively\n",
    "cleaned_2 = ''.join([char for char in sentence if char not in string.punctuation])\n",
    "print(\"\\nAlternative method:\")\n",
    "print(cleaned_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046781be-ecd0-468b-aed1-15eb48fb5197",
   "metadata": {},
   "source": [
    "# Exercise 3: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7b9de6-70db-4f65-8b35-c183c3b534cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /home/barraotieno/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download tokenizers\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8400bd04-b2fd-4302-94bd-dbe1ffd93da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences: ['Bitcoin is a cryptocurrency invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto.', 'The currency began use in 2009 when its implementation was released as open-source software.']\n",
      "\n",
      "Words: ['Bitcoin', 'is', 'a', 'cryptocurrency', 'invented', 'in', '2008', 'by', 'an', 'unknown'] ...\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"\"\"Bitcoin is a cryptocurrency invented in 2008 by an unknown person or group of people using the name Satoshi Nakamoto. The currency began use in 2009 when its implementation was released as open-source software.\"\"\"\n",
    "\n",
    "# Sentence tokenization\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentences:\", sentences)\n",
    "\n",
    "# Word tokenization\n",
    "words = word_tokenize(text)\n",
    "print(\"\\nWords:\", words[:10], \"...\")  # Show first 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a373afb-e38f-45d1-92a8-4228bbe2ee58",
   "metadata": {},
   "source": [
    "# EXERCISE 4: STOP WORDS REMOVAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680cb81e-a34f-4115-8151-f7d163683e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/barraotieno/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (only needed once)\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9c74e13-3acc-4fc8-91ec-60040fc0ebf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered tokens: ['goal', 'exercise', 'learn', 'remove', 'stop', 'words', 'nltk', 'stop', 'words', 'usually', 'refers', 'common', 'words', 'language']\n"
     ]
    }
   ],
   "source": [
    "# Sample text data\n",
    "text = \"\"\"\n",
    "The goal of this exercise is to learn to remove stop words with NLTK.  Stop words usually refers to the most common words in a language.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenize + lowercase\n",
    "tokens = word_tokenize(text.lower())\n",
    "\n",
    "# Load English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Filter\n",
    "filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "print(\"Filtered tokens:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0333c5f-58b1-4df3-80dc-6e20a00a2b92",
   "metadata": {},
   "source": [
    "# EXERCISE 5: STEMMING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84616bd9-61f2-4441-93e5-61664baec2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed tokens: ['the', 'interview', 'interview', 'the', 'presid', 'in', 'an', 'interview']\n"
     ]
    }
   ],
   "source": [
    "# sample text\n",
    "text = \"The interviewer interviews the president in an interview\"\n",
    "\n",
    "# Tokenize and stem\n",
    "tokens = word_tokenize(text.lower())\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "stemmed = [stemmer.stem(word) for word in tokens if word.isalpha()]\n",
    "print(\"Stemmed tokens:\", stemmed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6153022-196a-4d9a-a055-7ba44e2d957c",
   "metadata": {},
   "source": [
    "# EXERCISE 6: TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611482d6-cd20-4a72-923d-19649811e6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['edu', 'system', 'present', 'innov', 'curriculum', 'softwar', 'engin', 'program', 'renown', 'industrylead', 'reput', 'curriculum', 'rigor', 'design', 'learn', 'skill', 'digit', 'world', 'technolog', 'industri', 'take', 'differ', 'approach', 'classic', 'teach', 'method', 'today', 'learn', 'facilit', 'collect', 'cocr', 'process', 'profession', 'environ']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"\"\"\n",
    "01 Edu System presents an innovative curriculum in software engineering and programming. \n",
    "With a renowned industry-leading reputation, the curriculum has been rigorously designed for \n",
    "learning skills of the digital world and technology industry. Taking a different approach than \n",
    "the classic teaching methods today, learning is facilitated through a collective and co-creative \n",
    "process in a professional environment.\n",
    "\"\"\"\n",
    "\n",
    "# Define the preprocessing function\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Complete text preprocessing pipeline\n",
    "    Steps: Lowercase -> Remove Punctuation -> Tokenization -> Stopword Removal -> Stemming\n",
    "    \"\"\"\n",
    "    # 1. Lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Remove Punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 4. Stopword Filtering\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words and token.isalpha()]\n",
    "    \n",
    "    # 5. Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in filtered_tokens]\n",
    "    \n",
    "    return stemmed_tokens\n",
    "\n",
    "print(preprocess_text(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6686ec2-c204-438c-bee9-b67c73d383bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
